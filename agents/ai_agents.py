"""
AI Agents for LEAP Strategic Asset Engine
Uses OpenRouter API to access Grok and DeepSeek models
"""
import json
from typing import List, Optional, Dict, Any
import sys
from pathlib import Path
from datetime import datetime
import requests

sys.path.insert(0, str(Path(__file__).parent.parent))

import google.genai as genai

# Provider Registry - defines available LLM providers and their configurations
PROVIDER_REGISTRY = {
    "openrouter": {
        "name": "OpenRouter",
        "priority": 1,  # Highest priority (tried first)
        "client_class": lambda: OpenRouterClient(),
        "model": lambda: config.openrouter.deepseek_r1_model,
        "requires_key": lambda: config.openrouter.api_key,
        "free_tier": False,
        "supports_streaming": True,
    },
    "gemini": {
        "name": "Google Gemini", 
        "priority": 2,  # Fallback priority
        "client_class": lambda: GeminiClient(),
        "model": lambda: config.google_ai.model,
        "requires_key": lambda: config.google_ai.api_key,
        "free_tier": True,
        "supports_streaming": False,
    },
    "ollama": {
        "name": "Ollama (Local)",
        "priority": 3,  # Local fallback
        "client_class": lambda: OllamaClient(config.ollama.model),
        "model": lambda: config.ollama.model,
        "requires_key": lambda: True,  # Always available locally
        "free_tier": True,
        "supports_streaming": True,  # Enable streaming for all Ollama models
    },
    "ollama-deepseek-r1": {
        "name": "Ollama DeepSeek-R1",
        "priority": 4,  # Alternative local model
        "client_class": lambda: OllamaClient("deepseek-r1:latest"),
        "model": lambda: "deepseek-r1:latest",
        "requires_key": lambda: True,  # Always available locally
        "free_tier": True,
        "supports_streaming": True,  # Enable streaming
    },
    "ollama-deepseek-r1-32b": {
        "name": "Ollama DeepSeek-R1 32B (Slow)",
        "priority": 6,  # Lower priority due to slow response time
        "client_class": lambda: OllamaClient("deepseek-r1:32b"),
        "model": lambda: "deepseek-r1:32b",
        "requires_key": lambda: True,  # Always available locally
        "free_tier": True,
        "supports_streaming": True,  # Enable streaming for slow models
    },
    # Future providers can be added here with higher priorities
    # "anthropic": {...},
    # "openai": {...},
    # "groq": {...},
}

from quant_calculations import (
    black_scholes_price,
    extract_growth_assumptions,
    monte_carlo_itm_probability,
    verify_option_pricing
)
from models import (
    TrendingAsset, MarketData, OptionContract, 
    ReasoningOutput, ScenarioAnalysis, CritiqueOutput, RiskFlags
)
from config import config


class OpenRouterClient:
    """Base client for OpenRouter API calls"""
    
    def __init__(self):
        self.api_key = config.openrouter.api_key
        self.base_url = config.openrouter.base_url
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
            "HTTP-Referer": "https://github.com/leap-engine",  # Optional
            "X-Title": "LEAP Strategic Asset Engine"  # Optional
        }
    
    def call_llm(self, model: str, messages: List[Dict[str, str]], 
                 temperature: float = 0.7, max_tokens: int = 4000,
                 stream: bool = False, stream_callback: Optional[callable] = None) -> str:
        """
        Make an API call to OpenRouter
        
        Args:
            model: Model identifier (e.g., "x-ai/grok-beta")
            messages: List of message dicts with 'role' and 'content'
            temperature: Sampling temperature
            max_tokens: Maximum response tokens
            stream: Whether to stream the response
            stream_callback: Callback function for streaming chunks
            
        Returns:
            Model's response text
        """
        payload = {
            "model": model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "stream": stream
        }
        
        try:
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=60,
                stream=stream
            )
            response.raise_for_status()
            
            if stream and stream_callback:
                # Handle streaming response
                print("  ðŸ”„ Starting streaming response processing...")
                full_response = ""
                chunk_count = 0
                thought_buffer = ""  # Buffer for complete thoughts
                buffer_chunks = 0  # Count chunks in buffer
                
                for line in response.iter_lines():
                    if line:
                        chunk_count += 1
                        line = line.decode('utf-8')
                        if line.startswith('data: '):
                            if line.strip() == 'data: [DONE]':
                                # Send any remaining buffered thought
                                if thought_buffer.strip():
                                    stream_callback(thought_buffer)
                                    full_response += thought_buffer
                                print("  ðŸ Streaming complete")
                                break
                            try:
                                chunk = json.loads(line[6:])  # Remove 'data: ' prefix
                                if 'choices' in chunk and chunk['choices']:
                                    choice = chunk['choices'][0]
                                    if 'delta' in choice:
                                        delta = choice['delta']
                                        reasoning = delta.get('reasoning', '')
                                        content = delta.get('content', '')
                                        
                                        # DeepSeek R1 streams reasoning in 'reasoning' field, not 'content'
                                        if reasoning:
                                            thought_buffer += reasoning
                                            full_response += reasoning
                                            buffer_chunks += 1
                                            
                                            # Check for sentence endings, section breaks, or reasonable chunk limit
                                            if (any(end in reasoning for end in ['.', '!', '?', '\n\n', '**', '$$']) or 
                                                any(phrase in reasoning for phrase in ['Step ', 'Now ', "Let's ", 'First,', 'Next,', 'Finally,']) or
                                                buffer_chunks >= 200):
                                                stream_callback(thought_buffer)
                                                thought_buffer = ""
                                                buffer_chunks = 0
                                                
                                        elif content:
                                            thought_buffer += content
                                            full_response += content
                                            buffer_chunks += 1
                                            
                                            # Check for sentence endings, line breaks, markdown, or reasonable chunk limit for content
                                            if (any(end in content for end in ['.', '!', '?', '\n', '**', '$$', '\n\n']) or 
                                                buffer_chunks >= 100):
                                                stream_callback(thought_buffer)
                                                thought_buffer = ""
                                                buffer_chunks = 0
                            except json.JSONDecodeError:
                                continue
                
                print(f"  ðŸ“Š Total chunks processed: {chunk_count}")
                return full_response
            else:
                # Handle non-streaming response
                result = response.json()
                return result['choices'][0]['message']['content']
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ OpenRouter API error: {e}")
            raise


class DiscoveryScout:
    """AI agent for discovering trending assets using Grok/xAI or Gemini"""
    
    def __init__(self):
        self.client = LLMClient()
        self.model = config.openrouter.grok_model if config.openrouter.api_key else config.google_ai.model
    
    def discover_trending_assets(self, num_sectors: int = 5, 
                                 tickers_per_sector: int = 2) -> List[TrendingAsset]:
        """
        Discover trending assets across multiple sectors
        
        Args:
            num_sectors: Number of trending sectors to identify
            tickers_per_sector: Number of top tickers per sector
            
        Returns:
            List of TrendingAsset objects
        """
        # Add temporal constraints for backtesting mode
        current_date_context = datetime.now()
        temporal_constraint = ""
        
        if config.backtesting_mode and config.backtesting_date:
            current_date_context = config.backtesting_date
            temporal_constraint = f"""
**IMPORTANT: TEMPORAL CONSTRAINTS FOR HISTORICAL ANALYSIS**
- You are analyzing market conditions as they existed on {current_date_context.strftime('%Y-%m-%d')}
- You ONLY have access to information, news, and market data available up to this date
- You CANNOT use any knowledge of future events, earnings reports, or market developments after {current_date_context.strftime('%Y-%m-%d')}
- Base your analysis on what would have been known and visible to investors on this historical date
- Focus on trends, news, and market sentiment as they existed at that time
"""
        
        prompt = f"""You are a market research analyst scanning for high-conviction investment opportunities.

Task: Identify {num_sectors} trending sectors and {tickers_per_sector} top stock tickers per sector that are getting significant attention in financial news and social media (X/Twitter) recently.

For each ticker, provide:
1. The ticker symbol
2. The sector (e.g., "AI Infrastructure", "Semiconductors", "Clean Energy")
3. A concise narrative explaining why it's trending (macro tailwind, catalyst, sentiment)
4. A confidence score (0.0 to 1.0) based on the strength of the narrative

Return your response in JSON format:
```json
[
  {{
    "ticker": "NVDA",
    "sector": "AI Infrastructure",
    "narrative": "Dominant AI chip maker benefiting from enterprise AI adoption and data center buildout",
    "confidence_score": 0.85,
    "source": "X and financial news"
  }},
  ...
]
```

Focus on:
- Stocks with clear macro tailwinds (not just hype)
- Companies with institutional backing
- Assets suitable for LEAP options (liquid, established companies)
- Avoid penny stocks and illiquid names

{temporal_constraint}

Analysis Date: {current_date_context.strftime('%Y-%m-%d')}
"""
        
        messages = [
            {"role": "system", "content": "You are an expert market analyst specializing in identifying high-conviction investment themes."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = self.call_llm(self.model, messages, temperature=0.8)
            
            # Extract JSON from response
            json_str = self._extract_json(response)
            assets_data = json.loads(json_str)
            
            # Convert to TrendingAsset objects
            trending_assets = []
            for asset in assets_data:
                trending_assets.append(TrendingAsset(
                    ticker=asset['ticker'],
                    sector=asset['sector'],
                    narrative=asset['narrative'],
                    confidence_score=asset['confidence_score'],
                    source=asset.get('source', 'AI Discovery')
                ))
            
            print(f"âœ… Discovered {len(trending_assets)} trending assets")
            return trending_assets
            
        except Exception as e:
            print(f"âŒ Discovery error: {e}")
            return []


class GeminiClient:
    """Google AI (Gemini) client with real API integration"""
    
    def __init__(self):
        self.api_key = config.google_ai.api_key
        self.model = config.google_ai.model
        
        # Check if we should use real API or mock
        if self.api_key and self.api_key != 'your_google_ai_key_here':
            import google.generativeai as genai
            genai.configure(api_key=self.api_key)
            # Fix model name format
            model_name = self.model if self.model.startswith('models/') else f"models/{self.model}"
            self.client = genai.GenerativeModel(model_name)
            self.use_real_api = True
            print(f"ðŸ¤– Using REAL Gemini API with model: {model_name}")
        else:
            self.use_real_api = False
            print("ðŸ¤– Using MOCK Gemini client for development (free, no API calls)")
    
    def call_llm(self, messages: List[Dict[str, str]], 
                 temperature: float = 0.7, max_tokens: int = 4000,
                 stream: bool = False, stream_callback: Optional[callable] = None) -> str:
        """
        Call Gemini API or return mock responses
        """
        if self.use_real_api:
            try:
                # Extract the last user message
                user_messages = [msg for msg in messages if msg["role"] == "user"]
                last_message = user_messages[-1]["content"] if user_messages else ""
                
                # Configure generation parameters
                generation_config = {
                    "temperature": temperature,
                    "max_output_tokens": max_tokens,
                }
                
                # Call real Gemini API
                response = self.client.generate_content(
                    last_message,
                    generation_config=generation_config
                )
                
                return response.text
                
            except Exception as e:
                print(f"âš ï¸ Gemini API error: {e}")
                print("ðŸ”„ Falling back to mock responses...")
                self.use_real_api = False
                return self._get_mock_response(messages)
        else:
            return self._get_mock_response(messages)
    
    def _get_mock_response(self, messages: List[Dict[str, str]]) -> str:
        user_messages = [msg for msg in messages if msg["role"] == "user"]
        last_message = user_messages[-1]["content"] if user_messages else ""
        
        # Generate mock JSON responses based on content - return valid JSON
        if "break-even" in last_message.lower():
            return '''{
  "chain_of_thought": "Based on the LEAP option analysis, the break-even price at Day 365 is $152.40. This calculation considers the time decay of the option premium over the 12-month holding period. The stock would need to appreciate approximately 23.4% from the current price of $123.50 to reach this break-even level. The analysis incorporates Black-Scholes pricing verification showing the option is slightly underpriced at current market levels.",
  "break_even_price": 152.40,
  "scenario_analysis": {
    "bull_case": {"stock_price": 154.38, "option_value": 31.20, "return_pct": 24.8},
    "base_case": {"stock_price": 135.85, "option_value": 12.35, "return_pct": -50.6},
    "bear_case": {"stock_price": 104.98, "option_value": 0.00, "return_pct": -100.0}
  },
  "expected_value": 8.25,
  "expected_return_pct": 33.0,
  "max_loss": 25.00,
  "conviction_score": 0.72
}'''
            
        elif "scenario" in last_message.lower() or "bull case" in last_message.lower():
            return '''{
  "chain_of_thought": "Scenario analysis shows strong potential in the bull case with 35% probability weighting. The Monte Carlo simulation indicates 62.4% ITM probability with 95% confidence interval of 59.4%-65.4%. Black-Scholes verification confirms the option is underpriced by 9.4% compared to theoretical value. Risk/reward profile favors the trade given the growth assumptions extracted from the investment narrative.",
  "break_even_price": 152.40,
  "scenario_analysis": {
    "bull_case": {"stock_price": 154.38, "option_value": 31.20, "return_pct": 24.8},
    "base_case": {"stock_price": 135.85, "option_value": 12.35, "return_pct": -50.6},
    "bear_case": {"stock_price": 104.98, "option_value": 0.00, "return_pct": -100.0}
  },
  "expected_value": 8.25,
  "expected_return_pct": 33.0,
  "max_loss": 25.00,
  "conviction_score": 0.78
}'''
            
        elif "conviction" in last_message.lower() or "recommendation" in last_message.lower():
            return '''{
  "chain_of_thought": "After rigorous Chain-of-Thought analysis incorporating Black-Scholes pricing verification and Monte Carlo simulation: Key findings include pricing efficiency analysis showing 8.7% undervaluation, statistical probability assessment with 63.2% ITM likelihood (95% CI: 60.1%-66.3%), and favorable risk/reward profile. The investment thesis validation confirms growth assumptions are well-supported. Position sizing recommended at 2-3% of portfolio.",
  "break_even_price": 152.40,
  "scenario_analysis": {
    "bull_case": {"stock_price": 154.38, "option_value": 31.20, "return_pct": 24.8},
    "base_case": {"stock_price": 135.85, "option_value": 12.35, "return_pct": -50.6},
    "bear_case": {"stock_price": 104.98, "option_value": 0.00, "return_pct": -100.0}
  },
  "expected_value": 8.25,
  "expected_return_pct": 33.0,
  "max_loss": 25.00,
  "conviction_score": 0.78
}'''
            
        else:
            # Generic response for other queries
            return f'''{{
  "chain_of_thought": "Quantitative analysis completed using Black-Scholes pricing model and Monte Carlo simulation. The option shows fair pricing within acceptable bounds with {60 + (hash(last_message) % 20)}% probability of finishing in-the-money. Key considerations include strong delta positioning at 0.82, reasonable implied volatility at 48%, and sufficient time to expiration at 575 days for narrative development. The trade demonstrates positive expected value based on extracted growth assumptions from the investment thesis.",
  "break_even_price": 152.40,
  "scenario_analysis": {{
    "bull_case": {{"stock_price": 154.38, "option_value": 31.20, "return_pct": 24.8}},
    "base_case": {{"stock_price": 135.85, "option_value": 12.35, "return_pct": -50.6}},
    "bear_case": {{"stock_price": 104.98, "option_value": 0.00, "return_pct": -100.0}}
  }},
  "expected_value": 8.25,
  "expected_return_pct": 33.0,
  "max_loss": 25.00,
  "conviction_score": 0.75
}}'''


class OllamaClient:
    """Ollama local LLM client for offline inference"""
    
    def __init__(self, model: str = None):
        self.base_url = config.ollama.base_url
        self.model = model or config.ollama.model
        self.timeout = config.ollama.timeout
        
        # Check if Ollama is available
        try:
            import requests
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                if self.model in model_names:
                    print(f"ðŸ¤– Using Ollama with model: {self.model}")
                    self.available = True
                else:
                    print(f"âš ï¸ Model {self.model} not found. Available: {', '.join(model_names)}")
                    self.available = False
            else:
                print(f"âš ï¸ Ollama not responding at {self.base_url}")
                self.available = False
        except Exception as e:
            print(f"âš ï¸ Ollama connection error: {e}")
            self.available = False
    
    def call_llm(self, model: str = None, messages: List[Dict[str, str]] = None, 
                 temperature: float = 0.7, max_tokens: int = 4000,
                 stream: bool = False, stream_callback: Optional[callable] = None) -> str:
        """
        Call Ollama API locally with streaming support
        
        Args:
            model: Model name (ignored, uses self.model)
            messages: List of message dictionaries
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            stream: Whether to stream response
            stream_callback: Callback for streaming chunks
        """
        if not self.available:
            return "Ollama not available"
        
        if not messages:
            return "No messages provided"
        
        try:
            import requests
            
            # Extract the last user message for Ollama
            user_messages = [msg for msg in messages if msg["role"] == "user"]
            last_message = user_messages[-1]["content"] if user_messages else ""
            
            # Prepare Ollama API request
            payload = {
                "model": self.model,
                "prompt": last_message,
                "stream": stream  # Enable streaming
            }
            
            # Add options if provided
            options = {}
            if temperature is not None:
                options["temperature"] = float(temperature)
            if max_tokens is not None:
                options["num_predict"] = int(max_tokens)
            
            if options:
                payload["options"] = options
            
            if stream and stream_callback:
                # Streaming mode - get chunks and call callback
                response = requests.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    stream=True,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    full_response = ""
                    thinking_buffer = ""
                    
                    def flush_thinking():
                        nonlocal thinking_buffer
                        if thinking_buffer.strip():
                            # Clean up the thinking text - remove extra spaces and format nicely
                            clean_text = thinking_buffer.strip()
                            # Replace multiple spaces with single space
                            clean_text = ' '.join(clean_text.split())
                            stream_callback(f"ðŸ¤” {clean_text}")
                            thinking_buffer = ""
                    
                    for line in response.iter_lines():
                        if line:
                            try:
                                data = json.loads(line)
                                # Capture both response and thinking fields
                                if 'response' in data and data['response']:
                                    chunk = data['response']
                                    full_response += chunk
                                    stream_callback(chunk)
                                if 'thinking' in data and data['thinking']:
                                    chunk = data['thinking']
                                    full_response += chunk
                                    thinking_buffer += chunk
                                    
                                    # Output when we hit sentence boundaries or common punctuation
                                    if any(punct in thinking_buffer for punct in ['.', '!', '?', '\n']):
                                        flush_thinking()
                                    elif len(thinking_buffer) > 300:  # Flush if buffer gets too long
                                        flush_thinking()
                                    
                                if data.get('done', False):
                                    flush_thinking()  # Flush any remaining thinking
                                    break
                            except json.JSONDecodeError:
                                continue
                    
                    # After streaming, try to extract JSON from the full response
                    if '{' in full_response and '}' in full_response:
                        # Find the last JSON object in the response
                        start = full_response.rfind('{')
                        end = full_response.rfind('}') + 1
                        json_str = full_response[start:end]
                        try:
                            return json.loads(json_str)
                        except json.JSONDecodeError:
                            pass
                    
                    return full_response
                else:
                    return f"Ollama API error: {response.status_code}"
            else:
                # Non-streaming mode
                payload["stream"] = False
                response = requests.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    timeout=self.timeout
                )
                
                if response.status_code == 200:
                    try:
                        result = response.json()
                        # Check for response field first, then thinking field
                        if 'response' in result and result['response']:
                            response_content = result['response']
                            # Try to parse the response content as JSON
                            try:
                                parsed = json.loads(response_content)
                                return parsed
                            except json.JSONDecodeError:
                                # If not valid JSON, try to extract JSON from the text
                                if '{' in response_content and '}' in response_content:
                                    start = response_content.find('{')
                                    end = response_content.rfind('}') + 1
                                    json_str = response_content[start:end]
                                    try:
                                        return json.loads(json_str)
                                    except:
                                        pass
                                return response_content
                        elif 'thinking' in result and result['thinking']:
                            # If only thinking is present, the model is still reasoning
                            thinking_content = result['thinking']
                            # Try to extract JSON from thinking
                            try:
                                # First try to parse the entire thinking as JSON
                                parsed = json.loads(thinking_content)
                                return parsed
                            except json.JSONDecodeError:
                                # If that fails, try to find JSON within the thinking text
                                if '{' in thinking_content and '}' in thinking_content:
                                    start = thinking_content.find('{')
                                    end = thinking_content.rfind('}') + 1
                                    json_str = thinking_content[start:end]
                                    try:
                                        return json.loads(json_str)
                                    except:
                                        pass
                                # Return thinking text if no valid JSON found
                                return thinking_content[:1000] + "..." if len(thinking_content) > 1000 else thinking_content
                        else:
                            return result.get('response', '')
                    except json.JSONDecodeError as e:
                        # If the entire response isn't JSON, try to extract JSON from text
                        response_text = response.text
                        if '{' in response_text and '}' in response_text:
                            start = response_text.find('{')
                            end = response_text.rfind('}') + 1
                            json_str = response_text[start:end]
                            try:
                                return json.loads(json_str)
                            except:
                                pass
                        return response_text
                else:
                    print(f"âŒ Ollama API error: {response.status_code}")
                    return f"Ollama API error: {response.status_code}"
                    
        except Exception as e:
            print(f"âŒ Ollama call error: {e}")
            return f"Ollama call error: {e}"


class MultiProviderLLMClient:
    """Scalable LLM client that supports N providers with intelligent fallback"""
    
    def __init__(self, provider_ids: Optional[List[str]] = None):
        """
        Initialize with available providers
        
        Args:
            provider_ids: List of provider IDs to use (None = use all available)
        """
        self.providers = self._get_available_providers(provider_ids)
        self.current_provider = None
        
    def _get_available_providers(self, provider_ids: Optional[List[str]] = None) -> List[Dict]:
        """Get available providers sorted by priority"""
        available_providers = []
        
        # Filter providers if specific IDs requested
        providers_to_check = provider_ids if provider_ids else PROVIDER_REGISTRY.keys()
        
        for provider_id in providers_to_check:
            if provider_id in PROVIDER_REGISTRY:
                provider_config = PROVIDER_REGISTRY[provider_id].copy()
                provider_config["id"] = provider_id
                
                # Check if provider has required credentials
                if provider_config["requires_key"]():
                    available_providers.append(provider_config)
        
        # Sort by priority (lower number = higher priority)
        available_providers.sort(key=lambda x: x["priority"])
        
        return available_providers
    
    def call_llm_with_fallback(self, messages: List[Dict[str, str]], 
                              temperature: float = 0.7, max_tokens: int = 4000,
                              stream: bool = False, stream_callback: Optional[callable] = None,
                              excluded_providers: Optional[List[str]] = None) -> tuple[str, str]:
        """
        Try providers in priority order until one succeeds
        
        Returns:
            tuple: (response_text, provider_used)
        """
        excluded = excluded_providers or []
        
        for provider in self.providers:
            if provider["id"] in excluded:
                continue
                
            try:
                print(f"ðŸ”„ Trying {provider['name']}...")
                self.current_provider = provider
                
                # Create client instance
                client = provider["client_class"]()
                model = provider["model"]()
                
                # Use streaming if supported, otherwise fallback to non-streaming
                use_streaming = stream and provider["supports_streaming"]
                
                if provider["id"] == "openrouter":
                    response = client.call_llm(model, messages, temperature, max_tokens, 
                                             use_streaming, stream_callback)
                elif provider["id"].startswith("ollama"):
                    # All Ollama models expect model parameter first
                    response = client.call_llm(model, messages, temperature, max_tokens, 
                                              use_streaming, stream_callback)
                else:
                    # Gemini and other providers
                    response = client.call_llm(messages, temperature, max_tokens, 
                                              use_streaming, stream_callback)
                
                print(f"âœ… Success with {provider['name']}!")
                return response, provider["id"]
                
            except Exception as e:
                error_msg = str(e)
                print(f"âŒ {provider['name']} failed: {error_msg}")
                
                # Handle specific error types
                if "402" in error_msg and "Payment Required" in error_msg:
                    print(f"ðŸ’° {provider['name']} credits depleted - trying next provider...")
                elif "429" in error_msg:  # Rate limit
                    print(f"â±ï¸ {provider['name']} rate limited - trying next provider...")
                elif "401" in error_msg:  # Auth error
                    print(f"ðŸ” {provider['name']} auth failed - trying next provider...")
                # Continue to next provider
        
        # All providers failed
        raise Exception("All LLM providers failed - check API keys and network connection")
    
    def get_provider_info(self) -> Dict[str, Any]:
        """Get information about current provider setup"""
        return {
            "available_providers": [p["name"] for p in self.providers],
            "total_providers": len(self.providers),
            "current_provider": self.current_provider["name"] if self.current_provider else None,
            "free_tier_available": any(p["free_tier"] for p in self.providers),
        }


class LLMClient(MultiProviderLLMClient):
    """Backward-compatible LLM client for existing code"""
    
    def __init__(self, provider_ids: Optional[List[str]] = None):
        super().__init__(provider_ids)
        # For backward compatibility, use first available provider
        if self.providers:
            self.provider = self.providers[0]["id"]
        else:
            self.provider = None
    
    def call_llm(self, model: str, messages: List[Dict[str, str]], 
                 temperature: float = 0.7, max_tokens: int = 4000,
                 stream: bool = False, stream_callback: Optional[callable] = None) -> str:
        """Backward-compatible call method"""
        response, _ = self.call_llm_with_fallback(messages, temperature, max_tokens, stream, stream_callback)
        return response


class QuantReasoningEngine:
    """AI agent for quantitative LEAP analysis using DeepSeek R1 or Gemini"""
    
    def __init__(self):
        self.client = LLMClient()
        self.model = config.openrouter.deepseek_r1_model if config.openrouter.api_key else config.google_ai.model
        self.leap_heuristics = config.leap_heuristics
    
    def analyze_leap_opportunity(self, ticker: str, market_data: MarketData, 
                                 narrative: str) -> Optional[ReasoningOutput]:
        """
        Perform deep quantitative analysis on a LEAP candidate
        
        Args:
            ticker: Stock symbol
            market_data: Market data with option chain
            narrative: Discovery narrative explaining the thesis
            
        Returns:
            ReasoningOutput with recommendation and scenario analysis
        """
        if not market_data.option_contracts:
            print(f"âš ï¸ No suitable LEAP contracts found for {ticker}")
            return None
        
        # Select best contract (closest to 0.80 delta with longest expiry)
        best_contract = market_data.option_contracts[0]
        print(f"  ðŸ“Š Analyzing {ticker}: ${best_contract.strike} Call {best_contract.expiration.strftime('%Y-%m-%d')}")
        print(f"     Delta: {best_contract.delta:.3f} | Mid: ${((best_contract.bid + best_contract.ask) / 2):.2f}")
        print(f"     IV: {best_contract.implied_volatility:.2%} | Days: {best_contract.days_to_expiry}")
        
        # Extract growth assumptions from narrative
        print(f"  ðŸ“ˆ Extracting growth assumptions from narrative...")
        growth_assumptions = extract_growth_assumptions(narrative)
        print(f"     Expected Growth: {growth_assumptions['expected_growth']:.1%}")
        print(f"     Assumed Volatility: {growth_assumptions['volatility']:.1%}")
        
        # Perform Black-Scholes pricing verification
        print(f"  ðŸ’° Running Black-Scholes pricing verification...")
        market_price = (best_contract.bid + best_contract.ask) / 2
        T_years = best_contract.days_to_expiry / 365.0
        r = 0.05  # Risk-free rate assumption (5%)
        
        bs_price = black_scholes_price(
            S=market_data.spot_price,
            K=best_contract.strike,
            T=T_years,
            r=r,
            sigma=best_contract.implied_volatility or growth_assumptions['volatility']
        )
        
        pricing_verification = verify_option_pricing(market_price, bs_price)
        print(f"     Market Price: ${market_price:.2f} | BS Price: ${bs_price:.2f}")
        print(f"     Fairness: {pricing_verification['fairness_assessment']} ({pricing_verification['price_diff_pct']:+.1%})")
        
        # Run Monte Carlo simulation for ITM probability
        print(f"  ðŸŽ² Running Monte Carlo simulation (1,000 runs)...")
        mc_results = monte_carlo_itm_probability(
            S=market_data.spot_price,
            K=best_contract.strike,
            T=T_years,
            r=r,
            sigma=best_contract.implied_volatility or growth_assumptions['volatility'],
            growth_assumptions=growth_assumptions,
            n_runs=1000
        )
        
        print(f"     ITM Probability: {mc_results['itm_probability']:.1%} (95% CI: {mc_results['ci_lower']:.1%} - {mc_results['ci_upper']:.1%})")
        
        print(f"  ðŸ§  Building analysis prompt for {ticker}...")
        prompt = self._build_reasoning_prompt(ticker, market_data, best_contract, narrative, 
                                             growth_assumptions, pricing_verification, mc_results)
        print(f"  ðŸ¤– Calling DeepSeek R1 for quantitative reasoning...")
        
        messages = [
            {"role": "system", "content": "You are a quantitative analyst specializing in LEAP options strategies. Use rigorous Chain-of-Thought reasoning."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            print(f"  â³ Processing {ticker} analysis (this may take 30-60 seconds)...")
            
            # Define streaming callback to show chain of thought in real-time
            def stream_callback(chunk: str):
                # Clean up and format the chunk
                chunk = chunk.strip()
                if not chunk:
                    return

                # Buffer for collecting related chunks
                if not hasattr(stream_callback, 'buffer'):
                    stream_callback.buffer = []
                    stream_callback.last_section = None

                # Detect section changes and key insights
                if any(phrase in chunk.lower() for phrase in ['break-even', 'scenario', 'bull case', 'base case', 'bear case', 'expected value', 'risk assessment', 'conviction']):
                    # New section - flush buffer and show section header
                    if stream_callback.buffer:
                        _process_buffer()
                    section_name = chunk.split(':')[0] if ':' in chunk else chunk
                    print(f"\nðŸ” {section_name}...")
                    stream_callback.last_section = section_name
                    return

                elif any(chunk.endswith(end) for end in ['.', '!', '?']) and len(chunk) > 10:
                    # Complete sentence - show as insight
                    if stream_callback.buffer:
                        _process_buffer()
                    print(f"ðŸ’¡ {chunk}")
                    return

                elif '**' in chunk or '$$' in chunk:
                    # Structured content - show formatted
                    if stream_callback.buffer:
                        _process_buffer()
                    clean_chunk = chunk.replace('**', '').replace('$$', '').strip()
                    print(f"ðŸ“Š {clean_chunk}")
                    return

                elif any(char.isdigit() for char in chunk) and any(op in chunk for op in ['+', '-', '*', '/', '=']):
                    # Mathematical expressions - buffer and summarize
                    stream_callback.buffer.append(chunk)
                    # Show progress indicator every few chunks
                    if len(stream_callback.buffer) % 10 == 0:
                        print("âš¡ Calculating...", end='\r', flush=True)
                    return

                elif chunk and len(chunk) < 20 and not any(char.isdigit() for char in chunk):
                    # Short non-numeric chunks (likely words) - buffer for context
                    stream_callback.buffer.append(chunk)
                    return

                else:
                    # Other content - show as-is but clean
                    if stream_callback.buffer:
                        _process_buffer()
                    if len(chunk) > 5:  # Only show meaningful chunks
                        clean_chunk = chunk.replace('\\', '').replace('frac', '/')
                        print(f"ðŸ¤” {clean_chunk}", end='', flush=True)

            def _process_buffer():
                """Process accumulated buffer content"""
                if not stream_callback.buffer:
                    return

                buffer_text = ' '.join(stream_callback.buffer)

                # Try to extract key numbers or results
                import re
                numbers = re.findall(r'\d+\.?\d*', buffer_text)
                if len(numbers) >= 2 and stream_callback.last_section:
                    # Show summary for this section
                    if 'break' in stream_callback.last_section.lower():
                        print(f"ðŸ’° Break-even calculated")
                    elif 'bull' in stream_callback.last_section.lower():
                        print(f"ðŸ“ˆ Bull case: +{numbers[-1]}%" if numbers else "ðŸ“ˆ Bull case analyzed")
                    elif 'base' in stream_callback.last_section.lower():
                        print(f"âž¡ï¸ Base case: {numbers[-1]}%" if numbers else "âž¡ï¸ Base case analyzed")
                    elif 'bear' in stream_callback.last_section.lower():
                        print(f"ðŸ“‰ Bear case: {numbers[-1]}%" if numbers else "ðŸ“‰ Bear case analyzed")
                    elif 'expected' in stream_callback.last_section.lower():
                        print(f"ðŸŽ¯ Expected return: {numbers[-1]}%" if numbers else "ðŸŽ¯ Expected value calculated")

                stream_callback.buffer = []
            
            print("  ðŸ”„ Attempting streaming analysis...")
            response = self.client.call_llm(self.model, messages, temperature=0.3, max_tokens=6000, 
                                   stream=True, stream_callback=stream_callback)
            print(f"\n  âœ… Received analysis response for {ticker}")
            
            # Parse the structured response
            print(f"  ðŸ“‹ Parsing quantitative results for {ticker}...")
            reasoning_output = self._parse_reasoning_response(response, ticker, best_contract)
            
            print(f"âœ… Completed quant analysis for {ticker}")
            return reasoning_output
            
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ Initial provider failed: {error_msg}")
            
            # Use multi-provider fallback system
            try:
                print(f"ðŸ”„ Attempting fallback with other providers...")
                fallback_client = MultiProviderLLMClient()
                response, provider_used = fallback_client.call_llm_with_fallback(
                    messages, temperature=0.3, max_tokens=6000, 
                    stream=False, stream_callback=None  # Gemini doesn't support streaming
                )
                
                print(f"âœ… Fallback successful with {fallback_client.current_provider['name']}!")
                
                # Parse the structured response
                print(f"  ðŸ“‹ Parsing quantitative results for {ticker}...")
                reasoning_output = self._parse_reasoning_response(response, ticker, best_contract)
                
                print(f"âœ… Completed quant analysis for {ticker} (via {provider_used} fallback)")
                return reasoning_output
                
            except Exception as fallback_error:
                print(f"âŒ All providers failed: {fallback_error}")
                return None
    
    def _build_reasoning_prompt(self, ticker: str, market_data: MarketData,
                                contract: OptionContract, narrative: str,
                                growth_assumptions: Dict, pricing_verification: Dict,
                                mc_results: Dict) -> str:
        """Build the detailed reasoning prompt with quantitative data"""
        
        # Calculate time to expiration in years
        T_years = contract.days_to_expiry / 365.0
        
        # Add temporal constraints for backtesting mode
        analysis_date = datetime.now()
        temporal_constraint = ""
        
        if config.backtesting_mode and config.backtesting_date:
            analysis_date = config.backtesting_date
            temporal_constraint = f"""
**CRITICAL: TEMPORAL CONSTRAINTS FOR HISTORICAL ANALYSIS**
- You are performing this analysis on {analysis_date.strftime('%Y-%m-%d')}
- You ONLY have access to market data, news, and information available up to this historical date
- You CANNOT use any knowledge of future events, earnings results, or market developments after {analysis_date.strftime('%Y-%m-%d')}
- Base your reasoning on quantitative data and assumptions that would have been available to investors on this date
- The growth assumptions provided were extracted from narratives visible at that time
- Focus on statistical analysis using the provided quantitative inputs
"""
        
        prompt = f"""# LEAP Options Analysis: {ticker}

## Investment Thesis
{narrative}

## Quantitative Foundation

### Black-Scholes Pricing Verification
- **Market Price**: ${pricing_verification['market_price']:.2f}
- **Black-Scholes Price**: ${pricing_verification['bs_price']:.2f}
- **Price Difference**: {pricing_verification['price_diff_pct']:+.1%}
- **Assessment**: {pricing_verification['fairness_assessment']}

### Monte Carlo ITM Probability (1,000 Simulations)
- **ITM Probability**: {mc_results['itm_probability']:.1%} (95% CI: {mc_results['ci_lower']:.1%} - {mc_results['ci_upper']:.1%})
- **Expected ITM Value**: ${mc_results['expected_itm_value']:.2f}
- **Drift Rate**: {mc_results['drift_rate']:.1%}
- **Volatility Used**: {mc_results['simulated_volatility']:.1%}

### Growth Assumptions Extracted
- **Expected Annual Growth**: {growth_assumptions['expected_growth']:.1%}
- **Volatility Assumption**: {growth_assumptions['volatility']:.1%}

## Current Market Data
- **Spot Price**: ${market_data.spot_price:.2f}
- **52-Week Range**: ${market_data.price_52w_low:.2f} - ${market_data.price_52w_high:.2f}
- **IV Rank**: {market_data.implied_volatility_rank or 'N/A'}

## Proposed LEAP Contract
- **Strike**: ${contract.strike:.2f}
- **Expiration**: {contract.expiration.strftime('%Y-%m-%d')} ({contract.days_to_expiry} days)
- **Delta**: {contract.delta:.3f}
- **Mid Price**: ${((contract.bid + contract.ask) / 2):.2f}
- **Bid/Ask Spread**: {contract.bid_ask_spread_pct:.2f}%
- **Implied Volatility**: {contract.implied_volatility:.2%}

{temporal_constraint}

## Your Task
Perform a rigorous Chain-of-Thought analysis to determine if this is a sound LEAP trade using the "Buy 1.5yr, Sell 1yr" strategy.

### Analysis Framework:
1. **Thesis Validation**: Assess the strength of the investment narrative and growth assumptions
2. **Pricing Efficiency**: Evaluate if the option is fairly priced using Black-Scholes verification
3. **Probability Assessment**: Incorporate Monte Carlo ITM probability into your analysis
4. **Risk/Reward Profile**: Consider the statistical probability of success vs. premium paid

### Required Analysis:
1. **Break-Even Calculation**: What stock price is needed at Day 365 to break even?

2. **Scenario Analysis**: Estimate the option's value at the 1-year mark (Day 365) under three scenarios:
   - **Bull Case**: Stock price using the extracted growth assumptions
   - **Base Case**: Moderate growth scenario
   - **Bear Case**: Downside scenario

3. **Expected Value**: Calculate probability-weighted expected return using Monte Carlo results

4. **Conviction Score**: Rate 0.0 to 1.0 based on:
   - Thesis strength and growth assumptions
   - Options pricing efficiency (Black-Scholes verification)
   - Statistical probability of success (Monte Carlo ITM probability)
   - Risk/reward profile

## Output Format
Provide your analysis in JSON:
```json
{{
  "chain_of_thought": "Your detailed reasoning incorporating Black-Scholes verification and Monte Carlo probability...",
  "break_even_price": 150.00,
  "scenario_analysis": {{
    "bull_case": {{"stock_price": 180.00, "option_value": 35.00, "return_pct": 40.0}},
    "base_case": {{"stock_price": 157.50, "option_value": 28.00, "return_pct": 12.0}},
    "bear_case": {{"stock_price": 127.50, "option_value": 15.00, "return_pct": -40.0}}
  }},
  "expected_value": 26.50,
  "expected_return_pct": 6.0,
  "max_loss": 25.00,
  "conviction_score": 0.75
}}
```

Be thorough in your reasoning and incorporate the quantitative data provided.

Analysis Date: {analysis_date.strftime('%Y-%m-%d')}
"""
        return prompt
    
    def _parse_reasoning_response(self, response: str, ticker: str, 
                                  contract: OptionContract) -> ReasoningOutput:
        """Parse the JSON response from the reasoning engine"""
        try:
            json_str = self._extract_json(response)
            data = json.loads(json_str)
            
            # Build scenario analysis
            scenario_analysis = ScenarioAnalysis(
                bull_case=data['scenario_analysis']['bull_case'],
                base_case=data['scenario_analysis']['base_case'],
                bear_case=data['scenario_analysis']['bear_case'],
                expected_value=data['expected_value'],
                expected_return_pct=data['expected_return_pct'],
                risk_reward_ratio=abs(data['expected_return_pct'] / (data.get('max_loss', 1)))
            )
            
            return ReasoningOutput(
                ticker=ticker,
                recommended_contract=contract,
                scenario_analysis=scenario_analysis,
                chain_of_thought=data['chain_of_thought'],
                conviction_score=data['conviction_score'],
                break_even_price=data.get('break_even_price'),
                max_loss=data.get('max_loss'),
                projected_1yr_value=data['expected_value']
            )
            
        except Exception as e:
            print(f"âš ï¸ Error parsing reasoning response: {e}")
            # Return minimal output
            return ReasoningOutput(
                ticker=ticker,
                recommended_contract=contract,
                chain_of_thought=response,
                conviction_score=0.5
            )
    
    def _extract_json(self, text: str) -> str:
        """Extract JSON from response"""
        if "```json" in text:
            start = text.find("```json") + 7
            end = text.find("```", start)
            return text[start:end].strip()
        elif "```" in text:
            start = text.find("```") + 3
            end = text.find("```", start)
            return text[start:end].strip()
        return text.strip()


class RiskCritic(LLMClient):
    """
    The "Safety" - plays devil's advocate using DeepSeek V3
    Identifies red flags and provides risk assessment
    """
    
    def __init__(self):
        # Initialize parent class to get providers attribute
        super().__init__()
        # Override model selection
        self.model = config.openrouter.deepseek_v3_model if config.openrouter.api_key else config.google_ai.model
        self.leap_heuristics = config.leap_heuristics
    
    def critique_recommendation(self, ticker: str, reasoning: ReasoningOutput,
                               market_data: MarketData, earnings_date: Optional[datetime] = None) -> CritiqueOutput:
        """
        Perform critical risk assessment on a LEAP recommendation
        
        Args:
            ticker: Stock symbol
            reasoning: Output from QuantReasoningEngine
            market_data: Market data
            earnings_date: Next earnings date if available
            
        Returns:
            CritiqueOutput with risk flags and approval decision
        """
        prompt = self._build_critique_prompt(ticker, reasoning, market_data, earnings_date)
        
        messages = [
            {"role": "system", "content": "You are a risk management expert. Your job is to find problems and red flags in investment recommendations."},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = self.call_llm(self.model, messages, temperature=0.2)
            
            critique_output = self._parse_critique_response(response, ticker, reasoning, market_data, earnings_date)
            
            print(f"âœ… Risk critique completed for {ticker}")
            return critique_output
            
        except Exception as e:
            print(f"âŒ Critique error for {ticker}: {e}")
            # Default to cautious stance
            return CritiqueOutput(
                ticker=ticker,
                approved=False,
                risk_flags=RiskFlags(critical_issues=["Error during critique"]),
                critique_reasoning=str(e),
                risk_score=1.0
            )
    
    def _build_critique_prompt(self, ticker: str, reasoning: ReasoningOutput,
                              market_data: MarketData, earnings_date: Optional[datetime]) -> str:
        """Build critique prompt for risk assessment"""
        contract = reasoning.recommended_contract

        # Safely format values that might be None
        iv_display = f"{contract.implied_volatility:.2%}" if contract.implied_volatility is not None else "Unknown"
        spread_display = f"{contract.bid_ask_spread_pct:.2f}%" if contract.bid_ask_spread_pct is not None else "Unknown%"
        break_even_display = f"${reasoning.break_even_price:.2f}" if reasoning.break_even_price is not None else "Unknown"
        
        # Add temporal constraints for backtesting mode
        analysis_date = datetime.now()
        temporal_constraint = ""
        
        if config.backtesting_mode and config.backtesting_date:
            analysis_date = config.backtesting_date
            temporal_constraint = f"""
**CRITICAL: TEMPORAL CONSTRAINTS FOR HISTORICAL RISK ASSESSMENT**
- You are performing this risk assessment on {analysis_date.strftime('%Y-%m-%d')}
- You ONLY have access to market conditions and information available up to this historical date
- You CANNOT consider future events, earnings results, or market developments after {analysis_date.strftime('%Y-%m-%d')}
- Base your risk analysis on what would have been visible to investors on this date
- Consider historical volatility patterns and market conditions up to this point
"""
        
        prompt = f"""# Risk Assessment: {ticker} LEAP Trade

## Proposed Trade
- **Ticker**: {ticker}
- **Strike**: ${contract.strike:.2f}
- **Expiration**: {contract.expiration.strftime('%Y-%m-%d')}
- **Entry Price**: ${((contract.bid + contract.ask) / 2):.2f}
- **Delta**: {contract.delta:.3f}
- **IV**: {iv_display}
- **Bid/Ask Spread**: {spread_display}

## Quant's Conviction
- **Score**: {reasoning.conviction_score}/1.0
- **Expected Return**: {reasoning.scenario_analysis.expected_return_pct if reasoning.scenario_analysis else 'N/A'}%
- **Break-Even**: {break_even_display}

## Market Context
- **IV Rank**: {market_data.implied_volatility_rank or 'Unknown'}
- **Next Earnings**: {earnings_date.strftime('%Y-%m-%d') if earnings_date else 'Unknown'}

{temporal_constraint}

## Your Task: Play Devil's Advocate
Identify RED FLAGS and potential issues with this trade:

1. **IV Risk**: Is IV too high? (Reject if IV > 40th percentile)
2. **Liquidity Risk**: Is spread too wide or volume too low?
3. **Theta Risk**: Will time decay be too aggressive?
4. **Earnings Risk**: Is earnings within the hold period?
5. **Leverage Risk**: Is the position too risky given market conditions?

## Provide Output in JSON:
```json
{{
  "approved": true/false,
  "risk_score": 0.0 to 1.0,
  "critique_reasoning": "Detailed explanation of concerns...",
  "risk_flags": {{
    "high_iv": true/false,
    "low_liquidity": true/false,
    "wide_spread": true/false,
    "upcoming_earnings": true/false,
    "insufficient_time_value": true/false
  }},
  "critical_issues": ["issue 1", "issue 2"],
  "warnings": ["warning 1"]
}}
```

Be harsh. Only approve if this is a genuinely sound trade.

Analysis Date: {analysis_date.strftime('%Y-%m-%d')}
"""
        return prompt
    
    def _parse_critique_response(self, response: str, ticker: str, reasoning: ReasoningOutput,
                                 market_data: MarketData, earnings_date: Optional[datetime]) -> CritiqueOutput:
        """Parse critique response"""
        try:
            json_str = response
            if "```json" in response:
                start = response.find("```json") + 7
                end = response.find("```", start)
                json_str = response[start:end].strip()
            elif "```" in response:
                start = response.find("```") + 3
                end = response.find("```", start)
                json_str = response[start:end].strip()
            
            data = json.loads(json_str)
            
            # Build risk flags
            risk_flags_data = data.get('risk_flags', {})
            risk_flags = RiskFlags(
                high_iv=risk_flags_data.get('high_iv', False),
                low_liquidity=risk_flags_data.get('low_liquidity', False),
                wide_spread=risk_flags_data.get('wide_spread', False),
                upcoming_earnings=risk_flags_data.get('upcoming_earnings', False),
                insufficient_time_value=risk_flags_data.get('insufficient_time_value', False),
                critical_issues=data.get('critical_issues', []),
                warnings=data.get('warnings', [])
            )
            
            return CritiqueOutput(
                ticker=ticker,
                approved=data.get('approved', False),
                risk_flags=risk_flags,
                critique_reasoning=data.get('critique_reasoning', response),
                risk_score=data.get('risk_score', 0.5)
            )
            
        except Exception as e:
            print(f"âš ï¸ Error parsing critique: {e}")
            # Default to rejection
            return CritiqueOutput(
                ticker=ticker,
                approved=False,
                risk_flags=RiskFlags(critical_issues=["Parse error"]),
                critique_reasoning=response,
                risk_score=1.0
            )
